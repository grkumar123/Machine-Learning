{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Decision Tree - Classification (Scratch)**"
      ],
      "metadata": {
        "id": "G5Vuq6XJJpcE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Threshold Criteria for Decision Tree**\n",
        "1. Mean\n",
        "  - Mean Value: Similar to the median, you can use the mean (average) of the feature values as the threshold. However, this method is more sensitive to outliers.\n",
        "2. Percentiles\n",
        "  - Specific Percentiles: You can select thresholds based on specific percentiles (e.g., 25th, 75th). This can be useful for certain applications where you want to create splits at specific intervals.\n",
        "3. Custom Criteria\n",
        "  - Domain Knowledge: Sometimes, domain knowledge can guide the selection of thresholds based on meaningful values specific to the dataset (e.g., physiological thresholds in medical data).\n",
        "4. Statistical Tests\n",
        "  - Statistical Tests: Using statistical methods like t-tests or ANOVA to determine significant differences between groups can help identify potential thresholds.\n",
        "5. Quantile Thresholds\n",
        "  - Equally Sized Groups: Divide the data into quantiles (e.g., quartiles or quintiles) to determine thresholds that create equally sized groups, which can provide a balanced split.\n",
        "6. Entropy-Based or Gini-Based Thresholds\n",
        "  - Recursive Evaluation: Instead of using a single median or mean, you can evaluate each possible value for a feature in a range of values and calculate the information gain (or Gini impurity reduction) for each threshold. The threshold yielding the highest information gain is selected.\n",
        "7. Tree Ensemble Methods\n",
        "  - Bootstrap Aggregation: In methods like Random Forest, multiple trees are built, and thresholds are averaged across trees to create a more robust model.\n",
        "8. Dynamic Thresholds\n",
        "  - Adapt to Data: In some advanced algorithms (like certain variations of boosting or neural networks), thresholds can be dynamically adjusted based on model performance metrics rather than being statically chosen.\n",
        "9. Grid Search for Optimal Thresholds\n",
        "  - Parameter Optimization: In machine learning pipelines, grid search or other optimization techniques can be used to find the best threshold that maximizes a performance metric (like accuracy, F1 score, etc.) on a validation set."
      ],
      "metadata": {
        "id": "iGZBqeGTJnCD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperparameters for Decision Trees**\n",
        "- Criterion:\n",
        "  - Type: String\n",
        "  - Description: The function to measure the quality of a split.\n",
        "  - Options:\n",
        "    - 'gini': Gini impurity (for classification).\n",
        "    - 'entropy': Information gain (for classification).\n",
        "    - 'squared_error': Mean squared error (for regression).\n",
        "    - 'friedman_mse': Friedman’s mean squared error (for regression).\n",
        "    - 'absolute_error': Mean absolute error (for regression).\n",
        "    - 'poisson': Poisson deviance (for regression).\n",
        "- Max Depth:\n",
        "  - Type: Integer or None\n",
        "  - Description: The maximum depth of the tree.\n",
        "  - Default: None (nodes are expanded until all leaves are pure or contain fewer than min_samples_split samples).\n",
        "- Min Samples Split:\n",
        "  - Type: Integer or Float\n",
        "  - Description: The minimum number of samples required to split an internal node.\n",
        "  - Default: 2.\n",
        "- Min Samples Leaf:\n",
        "  - Type: Integer or Float\n",
        "  - Description: The minimum number of samples that must be present in a leaf node.\n",
        "  - Default: 1.\n",
        "- Max Features:\n",
        "  - Type: Integer, Float, String, or None\n",
        "  - Description: The number of features to consider when looking for the best split.\n",
        "  - Options:\n",
        "    - Integer: Consider max_features features at each split.\n",
        "    - Float: Consider a fraction of the total number of features.\n",
        "    - 'auto': Uses sqrt(n_features) for classification and n_features for regression.\n",
        "    - 'sqrt': Same as 'auto'.\n",
        "    - 'log2': Consider log2(n_features) features.\n",
        "    - None: Consider all features.\n",
        "- Max Leaf Nodes:\n",
        "  - Type: Integer or None\n",
        "  - Description: Limits the number of leaf nodes in the tree.\n",
        "  - Default: None.\n",
        "- Min Impurity Decrease:\n",
        "  - Type: Float\n",
        "  - Description: A node will be split if this value is greater than or equal to the impurity decrease of the node.\n",
        "  - Default: 0.0.\n",
        "- Class Weight (for DecisionTreeClassifier):\n",
        "  - Type: Dict, ‘balanced’, or None\n",
        "  - Description: Weights associated with classes in classification problems.\n",
        "  - Default: None (all classes are supposed to have weight one).\n",
        "- Presort (deprecated in newer versions of scikit-learn):\n",
        "  - Type: Boolean\n",
        "  - Description: Whether to presort the data to speed up the finding of the best splits.\n",
        "  - Default: False.\n",
        "- Random State:\n",
        "  - Type: Integer or None\n",
        "  - Description: Controls the randomness of the estimator.\n",
        "  - Default: None."
      ],
      "metadata": {
        "id": "UWFdGjtwJ1BU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stopping Criteria for Decision Trees**\n",
        "- Maximum Depth: Specify a maximum depth for the tree. This limits how deep the tree can grow, preventing it from becoming overly complex.\n",
        "- Minimum Samples per Leaf: Set a minimum number of samples (rows) required to be present in a leaf node. If a node has fewer samples than this threshold, it will not be split further.\n",
        "- Minimum Information Gain: Define a minimum threshold for information gain. If the information gain from a potential split is less than this threshold, the node will not be split.\n",
        "- Maximum Number of Leaves: Limit the total number of leaf nodes in the tree. This can help control the complexity of the model.\n",
        "- Pure Node (Homogeneity): Stop splitting when a node is pure, meaning all samples in the node belong to the same class. At this point, no further splits will improve classification.\n",
        "- Insufficient Gain: If further splits do not provide a significant reduction in impurity (e.g., information gain or Gini impurity), the splitting process can be terminated.\n",
        "- Predefined Class Distribution: If the distribution of classes in a node reaches a predefined condition (e.g., a specific ratio of classes), further splitting can be stopped."
      ],
      "metadata": {
        "id": "EElXfPtjJ8Ib"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libraries**"
      ],
      "metadata": {
        "id": "BpE8MihsHtb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_diabetes, load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, explained_variance_score\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc"
      ],
      "metadata": {
        "id": "_LDT-yN3Hsif"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None, leaf=False, class_value=None):\n",
        "        self.feature = feature          # Feature index for splitting\n",
        "        self.threshold = threshold      # Threshold value for splitting\n",
        "        self.left = left                # Left child node\n",
        "        self.right = right              # Right child node\n",
        "        self.leaf = leaf                # Whether this node is a leaf\n",
        "        self.class_value = class_value  # Class label if it's a leaf"
      ],
      "metadata": {
        "id": "eE__1nWTHzXM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecisionTree:\n",
        "    def __init__(self, criterion='gini', max_depth=None, min_samples_split=2, loss_function='categorical_cross_entropy'):\n",
        "        self.criterion = criterion\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.loss_function = loss_function\n",
        "        self.tree = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.tree = self._build_tree(X, y)\n",
        "\n",
        "    def _build_tree(self, X, y, depth=0):\n",
        "        # Stopping criteria\n",
        "        if len(y) < self.min_samples_split or (self.max_depth and depth >= self.max_depth) or len(set(y)) == 1:\n",
        "            return self._create_leaf_node(y)\n",
        "\n",
        "        # Find the best split\n",
        "        best_feature, best_threshold = self._find_best_split(X, y)\n",
        "        if best_feature is None:\n",
        "            return self._create_leaf_node(y)\n",
        "\n",
        "        # Split the data\n",
        "        left_indices = X[:, best_feature] < best_threshold\n",
        "        right_indices = X[:, best_feature] >= best_threshold\n",
        "\n",
        "        left_child = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
        "        right_child = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
        "\n",
        "        return Node(feature=best_feature, threshold=best_threshold, left=left_child, right=right_child)\n",
        "\n",
        "    def _find_best_split(self, X, y):\n",
        "        best_gain = -np.inf\n",
        "        best_feature = None\n",
        "        best_threshold = None\n",
        "        n_features = X.shape[1]\n",
        "\n",
        "        for feature in range(n_features):\n",
        "            thresholds = np.unique(X[:, feature])\n",
        "            for threshold in thresholds:\n",
        "                gain = self._information_gain(X, y, feature, threshold)\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    best_feature = feature\n",
        "                    best_threshold = threshold\n",
        "\n",
        "        return best_feature, best_threshold\n",
        "\n",
        "    def _information_gain(self, X, y, feature, threshold):\n",
        "        # Calculate the impurity before the split\n",
        "        parent_impurity = self._impurity(y)\n",
        "\n",
        "        # Split the data\n",
        "        left_indices = X[:, feature] < threshold\n",
        "        right_indices = X[:, feature] >= threshold\n",
        "\n",
        "        # Calculate the weighted impurity of the children\n",
        "        n = len(y)\n",
        "        n_left = np.sum(left_indices)\n",
        "        n_right = np.sum(right_indices)\n",
        "\n",
        "        if n_left == 0 or n_right == 0:\n",
        "            return 0\n",
        "\n",
        "        child_impurity = (n_left / n) * self._impurity(y[left_indices]) + (n_right / n) * self._impurity(y[right_indices])\n",
        "\n",
        "        # Calculate information gain\n",
        "        return parent_impurity - child_impurity\n",
        "\n",
        "    def _impurity(self, y):\n",
        "        if self.criterion == 'gini':\n",
        "            return self._gini_impurity(y)\n",
        "        elif self.criterion == 'entropy':\n",
        "            return self._entropy(y)\n",
        "\n",
        "    def _gini_impurity(self, y):\n",
        "        classes, counts = np.unique(y, return_counts=True)\n",
        "        total = len(y)\n",
        "        return 1 - sum((count / total) ** 2 for count in counts)\n",
        "\n",
        "    def _entropy(self, y):\n",
        "        classes, counts = np.unique(y, return_counts=True)\n",
        "        total = len(y)\n",
        "        return -sum((count / total) * np.log2(count / total) for count in counts if count > 0)\n",
        "\n",
        "    def _create_leaf_node(self, y):\n",
        "        most_common = np.bincount(y).argmax()\n",
        "        return Node(leaf=True, class_value=most_common)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._predict_sample(sample, self.tree) for sample in X])\n",
        "\n",
        "    def _predict_sample(self, sample, tree):\n",
        "        if tree.leaf:\n",
        "            return tree.class_value\n",
        "\n",
        "        if sample[tree.feature] < tree.threshold:\n",
        "            return self._predict_sample(sample, tree.left)\n",
        "        else:\n",
        "            return self._predict_sample(sample, tree.right)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"Predict class probabilities for the input samples.\"\"\"\n",
        "        probs = []\n",
        "        for sample in X:\n",
        "            class_counts = [0] * (np.max(y) + 1)  # Adjust based on number of classes\n",
        "            self._count_classes(sample, self.tree, class_counts)\n",
        "            total = sum(class_counts)\n",
        "            probs.append([count / total for count in class_counts])  # Convert counts to probabilities\n",
        "        return np.array(probs)\n",
        "\n",
        "    def _count_classes(self, sample, tree, class_counts):\n",
        "        if tree.leaf:\n",
        "            class_counts[tree.class_value] += 1\n",
        "            return\n",
        "\n",
        "        if sample[tree.feature] < tree.threshold:\n",
        "            self._count_classes(sample, tree.left, class_counts)\n",
        "        else:\n",
        "            self._count_classes(sample, tree.right, class_counts)\n",
        "\n",
        "    def binary_cross_entropy(self, y_true, y_pred):\n",
        "        epsilon = 1e-9\n",
        "        y1 = y_true * np.log(y_pred + epsilon)\n",
        "        y2 = (1 - y_true) * np.log(1 - y_pred + epsilon)\n",
        "        return -np.mean(y1 + y2)\n",
        "\n",
        "    def categorical_cross_entropy(self, y_true, y_pred):\n",
        "        epsilon = 1e-9\n",
        "        # Assuming y_true is one-hot encoded\n",
        "        return -np.mean(np.sum(y_true * np.log(y_pred + epsilon), axis=1))\n",
        "\n",
        "    def calculate_loss(self, y_true, predictions_proba):\n",
        "        \"\"\"Calculate the loss based on the selected loss function.\"\"\"\n",
        "        if self.loss_function == 'binary_cross_entropy':\n",
        "            return self.binary_cross_entropy(y_true, predictions_proba[:, 1])  # Assuming binary class 1\n",
        "        elif self.loss_function == 'categorical_cross_entropy':\n",
        "            # Convert y_true to one-hot encoding for categorical cross-entropy\n",
        "            y_true_one_hot = np.eye(np.max(y_true) + 1)[y_true]\n",
        "            return self.categorical_cross_entropy(y_true_one_hot, predictions_proba)\n",
        "\n",
        "    def print_tree(self, tree=None, indent=\"  \"):\n",
        "        \"\"\"Prints the structure of the decision tree\"\"\"\n",
        "        if tree is None:\n",
        "            tree = self.tree\n",
        "        if tree.leaf:\n",
        "            print(f\"{indent}Leaf: Class {tree.class_value}\")\n",
        "        else:\n",
        "            print(f\"{indent}Feature {tree.feature} <= {tree.threshold}\")\n",
        "            print(f\"{indent}Left:\")\n",
        "            self.print_tree(tree.left, indent + \"  \")\n",
        "            print(f\"{indent}Right:\")\n",
        "            self.print_tree(tree.right, indent + \"  \")\n",
        "\n",
        "    def accuracy(self, y, y_hat):\n",
        "        return np.sum(y == y_hat) / len(y)\n",
        "\n",
        "    def custom_metrics(self, y, y_hat):\n",
        "        tp, tn, fp, fn = 0, 0, 0, 0\n",
        "        for i in range(len(y)):\n",
        "            if y[i] == 1 and y_hat[i] == 1:\n",
        "                tp += 1\n",
        "            elif y[i] == 1 and y_hat[i] == 0:\n",
        "                fn += 1\n",
        "            elif y[i] == 0 and y_hat[i] == 1:\n",
        "                fp += 1\n",
        "            elif y[i] == 0 and y_hat[i] == 0:\n",
        "                tn += 1\n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        print(f\"Precision: {precision:.2f}\")\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        print(f\"Recall (Sensitivity): {recall:.2f}\")\n",
        "        f1_score = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0\n",
        "        return f1_score"
      ],
      "metadata": {
        "id": "O-0taih8H3TF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Dataset**"
      ],
      "metadata": {
        "id": "O1o2s6KNH9rB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target"
      ],
      "metadata": {
        "id": "es5NdMNGH_7X"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(X)), print(type(y))\n",
        "print(X.shape), print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4J8osgYJF37",
        "outputId": "45e8ac23-c302-45d9-a46b-2e75002c539e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "(150, 4)\n",
            "(150,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X[:5])\n",
        "print(y[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1aV_isJJOdq",
        "outputId": "ccf85da3-31ea-478a-8050-189b12adc30e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[5.1 3.5 1.4 0.2]\n",
            " [4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]\n",
            " [4.6 3.1 1.5 0.2]\n",
            " [5.  3.6 1.4 0.2]]\n",
            "[0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxOHztalICk5",
        "outputId": "ef68a60e-c8f6-4c9b-b68c-60a5a5514ed1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((120, 4), (30, 4), (120,), (30,))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "tree = DecisionTree(criterion='gini', max_depth=3, min_samples_split=2, loss_function='categorical_cross_entropy')\n",
        "tree.fit(X_train, y_train)\n",
        "print(\"Decision Tree Structure:\")\n",
        "tree.print_tree()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJ0dbrHKIG7A",
        "outputId": "a37c4d1d-f4e3-4bfb-c95a-b55faa4ba9a1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Structure:\n",
            "  Feature 2 <= 3.0\n",
            "  Left:\n",
            "    Leaf: Class 0\n",
            "  Right:\n",
            "    Feature 2 <= 4.8\n",
            "    Left:\n",
            "      Feature 3 <= 1.7\n",
            "      Left:\n",
            "        Leaf: Class 1\n",
            "      Right:\n",
            "        Leaf: Class 2\n",
            "    Right:\n",
            "      Feature 3 <= 1.8\n",
            "      Left:\n",
            "        Leaf: Class 1\n",
            "      Right:\n",
            "        Leaf: Class 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions\n",
        "predictions = tree.predict(X_test)\n",
        "print(\"Predictions:\", predictions)\n",
        "print(\"Actual:\", y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gy9d1jrfIS-m",
        "outputId": "ccf81d2e-7fef-4f69-b286-a80f252e5f85"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "Actual: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate probabilities for loss calculation\n",
        "predictions_proba = tree.predict_proba(X_test)\n",
        "print(\"Prediction Probabilities:\\n\", predictions_proba)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_9Elvg_ITAz",
        "outputId": "82becad7-af5a-4f6f-9322-c7cc8c4ac5d3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction Probabilities:\n",
            " [[0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss\n",
        "loss = tree.calculate_loss(y_test, predictions_proba)\n",
        "print(f\"Loss: {loss:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GU2sVdBrITDX",
        "outputId": "8e3f1910-43f0-4a04-fa4d-25e359d84831"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: -0.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy\n",
        "accuracy = tree.accuracy(y_test, predictions)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "f1_score = tree.custom_metrics(y_test, predictions)\n",
        "print(f\"F1 Score: {f1_score:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEpo--SbITGC",
        "outputId": "fcb53883-3228-4072-88e2-78fcfa12fb44"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.00\n",
            "Precision: 1.00\n",
            "Recall (Sensitivity): 1.00\n",
            "F1 Score: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inbuilt Functions\n",
        "# Accuracy and Confusion Matrix\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"Decision Tree Accuracy: {accuracy:.2f}\")\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, predictions))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNbQftzMI_6T",
        "outputId": "4416274d-56f9-47c4-e74b-50a1b16342e2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.00\n",
            "Confusion Matrix:\n",
            " [[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n"
          ]
        }
      ]
    }
  ]
}